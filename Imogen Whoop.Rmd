---
title: "Personal Whoop Analysis: What makes the Optimal Taper?"
author: "Imogen Meers"
date: "26/1/24"
output: pdf_document
---

# Motivation

Taper is one of the hardest but also most important parts of the swimming cycle to execute. Years of training can come to fruition with a good taper, but swimmers often fall short if they "miss" their taper.

This is an important problem to me personally as I am using my own Whoop data from my college experience (2020-2025) to decipher what are important features of taper for me and what I can optimize/avoid in order to have a "peak performance."

Whilst it will be useful to see how Whoop affects taper and competition, it is important to note that this analysis is a small snapshot of my actions that affect my swimming. I am not including injury, illness, nutrition or recovery data, which would help to draw a better picture of my performance. I am, however, hoping that significant changes in these areas will be reflected in my personal metrics and thus be considered.

# Previous Work

There have been a number of studies on wearable tech metrics and their accuracy and predictability for factors of performance, recovery and strain. 

One study I found that was particularly interesting and related to my analysis is the thesis of Emily Lundstrom, a Masters student at Penn State University called _Effectiveness of Wearable Technology for Predicting Measures of Metabolism and Performance in Collegiate Division 1 Swimmers_. She found a negative correlation between strain and time trial performance in female swimmers as well as a positive correlation between HRV and suppressed metabolic rate. These are both significant findings when looking at Whoop metrics in swimmers and are similar to what I would expect to find in my own analysis. 


# Problem Framing

After an exploratory analysis of the different data sets collected by Whoop, which include physiology, sleep and workouts, I intend to analyse different parts of this problem.

1. What factors contribute the most to my peak performances?
  For this, I will use both my own derived variables such as AC ratio, and Whoop's derived metrics to create a model that predicts performance and extract variable importance. 
  
  
2. How my performance varies between events, based on taper?
  For this, I will extract more data from my swims and look at distribution of splits and overall results for 50 vs 100 and how factors differ between good performances in front-end vs. back-end speed.


##Data Overview

My data sources will be:

Whoop metrics from July 2020 to Jan 2025

  - Whoop derived variables:
      - Physiological
      - Sleep
      - Workout
      
  - Self derived variables
      - AC Ratios
      - Rolling average
      - Workout description indicator variables
  
swimcloud.com meet results from July 2020 to Jan 2025

This data is generally pretty clean and comprehensive. I have worn my Whoop consistently for the past 4 years with only a few days missing data. I do take off my Whoop when I am competing, but this should not affect my analysis as I am using results-based data to evaluate success of a meet rather than in-meet Whoop metrics.

The swimcloud website is also a reliable data source. They report times for all meets and splits for most, meaning there is no need to impute data here.

## Bibliography

Lundstrom, E. (2020). Effectiveness of Wearable Technology for Predicting Measures of Metabolism and Performance in Collegiate Division 1 Swimmers. Master's Thesis, Pennsylvania State University1. Retrieved from Pennsylvania State University Electronic Theses and Dissertations.

# Preliminary EDA

Let's load in the packages we are going to use for the analysis:

```{r warning=FALSE}
library(ggplot2) # load ggplot2
library(ggdark) # load ggdark
library(data.table) # load data.table
library(dplyr)
library(lubridate)
library(tidyr)
library(stringr)
```



```{r}
phys_cycles <- read.csv("physiological_cycles.csv")
dim(phys_cycles) # View dimensions of the data
plot_data <- phys_cycles 
colnames(plot_data) <- c("cycle_start", "cycle_end", "cycle_tzone", "recovery", "rhr", "hrv", "skin_temp", "blood_oxygen", "day_strain", "energy_burned", "maxhr", "ahr", "sleep_onset", "wake_onset", "sleep_performance", "resp_rate", "asleep_dur", "in_bed_dur", "light_sleep", "deep_sleep", "rem", "awake_dur", "sleep_need", "sleep_debt", "sleep_efficiency", "sleep_consistency")

summary(plot_data) # Summarise metric data
```
This data has 1477 rows dating from July 2020 to present. It includes general data on physiological cycles including sleep and  daily metrics.

```{r}
  # Create a mapping of time zone offsets to Olson names
  tzone_mapping <- c(
    "UTC-05:00" = "America/New_York",
    "UTCZ" = "UTC",
    "UTC-06:00" = "America/Chicago",
    "UTC-07:00" = "America/Denver",
    "UTC-04:00" = "America/Halifax",
    "UTC+02:00" = "Europe/Bucharest",
    "UTC+01:00" = "Europe/Paris"
  )

  grade_mapping <- function(date) {
    #' This calculates grade based on date range
    #'
    #' @param date the date to find the grade from
    #' 
    #' @return grade as string
    #'
    #'
    
    
    
    if (date >= as.Date("2020-08-01") & date < as.Date("2021-05-01")) {
      return("Freshman")
    } else if (date >= as.Date("2021-05-01") & date < as.Date("2022-05-01")) {
      return("Sophomore")
    } else if (date >= as.Date("2022-05-01") & date < as.Date("2023-05-01")) {
      return("Junior")
    } else if (date >= as.Date("2023-05-01") & date < as.Date("2024-05-01")) {
      return("Senior")
    } else if (date >= as.Date("2024-05-01")) {
      return("Grad")
    } else {
      return(NA)  # In case date does not fall within any range
    }
  }




  days_since_mapping <- function(grade, date) {
    
  #' This calculates number of days since the start of the season depending on grade (rowwise)
  #'
  #' @param grade the grade as a string
  #' @param date the date to find the difference from
  #' 
  #' @return number of days from date
  #'
  #'
    if (is.na(grade)){
      return(NA)
    }
    
    
    if (grade == "Freshman") {
      reference_date = "2020-05-01"
      return(as.integer(difftime(date, reference_date, units = "days")))
    } else if (grade == "Sophomore") {
      reference_date = "2021-05-01"
      return(as.integer(difftime(date, reference_date, units = "days")))
    } else if (grade == "Junior") {
      reference_date = "2022-05-01"
      return(as.integer(difftime(date, reference_date, units = "days")))
    } else if (grade == "Senior") {
        reference_date = "2023-05-01"
        return(as.integer(difftime(date, reference_date, units = "days")))
    } else {
        reference_date = "2024-05-01"
        return(as.integer(difftime(date, reference_date, units = "days")))
    }
  }

date_cleaning <- function(plot_data){
  #'
  #' This function cleans up the dates, and assigns years and days since the start of the period
  #'
  #' @param plot_data the unclean df
  #' 
  #' @return A cleaned df with appropriate manipulated and added fields
  #'

  

  
  # Apply mapping of tzone
  plot_data <- plot_data %>%
    mutate(cycle_olson = tzone_mapping[cycle_tzone])
  
  # Convert to dates, but also converts tzone to UTC (don't want) so force_tz
  plot_data <- plot_data %>% rowwise() %>%
    mutate(cycle_start = force_tz(ymd_hms(cycle_start, tz = cycle_olson)),
           cycle_end = force_tz(ymd_hms(cycle_end, tz = cycle_olson)),
           wake_onset = force_tz(ymd_hms(wake_onset, tz = cycle_olson)),
           sleep_onset = force_tz(ymd_hms(sleep_onset, tz = cycle_olson)),) %>% ungroup() 
  
  

  
  plot_data <- plot_data %>% filter(!is.na(cycle_start) & !is.na(cycle_end) & !is.na(wake_onset))
  
  # Apply the function to the data frame
  plot_data <-  plot_data %>% mutate(grade = sapply(wake_onset, grade_mapping))
  summary(plot_data)
  

  # Apply the function to the data frame
  plot_data <-  plot_data %>% mutate(days_since = mapply(days_since_mapping, grade, as.Date(wake_onset)))
  
  #Remove time zone cols
  plot_data <- plot_data %>% select(-cycle_olson, -cycle_tzone)
  
  return(plot_data)

}

plot_data <- date_cleaning(plot_data)

melted_data <- plot_data %>% select(-c(cycle_start, cycle_end, sleep_onset )) %>% pivot_longer(cols = -c(wake_onset, grade, days_since), names_to = "metric", values_to= "value")


```

## Physiological Metrics over Years

```{r  warning=FALSE}

metric_plot <- melted_data %>% mutate(reference = ifelse(month(wake_onset) == 8, "pre season", ifelse(month(wake_onset) %in% c(5,6,7), "summer", ifelse(month(wake_onset) == 2, "conference", NA))))

metrics = c("rhr", "hrv", "maxhr", "recovery")
for (i in 1:4){
  g_metric <- ggplot(metric_plot[metric_plot$metric == metrics[i],], # Set data
              aes(x = days_since, y = value, color = grade))+ # Set aesthetics
  #geom_jitter(alpha = 0.2, height =0.1, aes(fill = metric))+ # Add points 
    
  geom_vline(xintercept = metric_plot[metric_plot$reference == "pre season",]$days_since, linetype = "dotted", color = "blue") + # Add vertical reference line
    
  geom_vline(xintercept = metric_plot[metric_plot$reference == "conference",]$days_since, linetype = "dotted", color = "green") + # Add vertical reference line
    
  geom_smooth(se = FALSE) + # Add smooting line
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + # Turn off grid
  dark_theme_minimal() + # Set theme
  labs(title = paste0("Whoop Metrics -- ", metrics[i]), # Set labels
       color = "Year") +
      annotate("text", x = 110, y = max(metric_plot[metric_plot$metric == metrics[i] & !is.na(metric_plot$value),]$value) -5, label = "pre season", color = "blue") + # Label for vertical line
    annotate("text", x = 290, y = max(metric_plot[metric_plot$metric == metrics[i] & !is.na(metric_plot$value),]$value) -5, label = "conference", color = "green") + # Label for vertical line
  guides(fill = "none") 

# Generate plot
print(g_metric)

}
# Turn off dark mode
invert_geom_defaults()

```


## Meet Overview

```{r}
all_swim <- read.csv("all_swim_results.csv")

```

```{r  warning=FALSE}
swim_df <- all_swim %>% mutate(Date = str_replace_all(Date, "–\\d{1,2}", ""), Results = str_replace_all(Results, "\\[|'|\\]|%|\\+", "")) %>% 
  mutate(Date = mdy(Date)) %>% 
    separate(Results, into = c("1", "2", "3"), sep = "\\), \\(", remove = TRUE) %>% 
      pivot_longer(cols = c("1", "2", "3"), names_to = "Event_Num", values_to = "Results")  %>% 
        mutate(Results = str_replace_all(Results, "\\(|\\)", "")) %>% 
          separate(Results, into = c("Event", "Time", "Season_Improvement"), sep=",") %>%
          mutate(Time = trimws(Time), Season_Improvement = trimws(Season_Improvement))  %>% 
            mutate(Season_Improvement = as.numeric(Season_Improvement))

imputed <- swim_df %>% mutate(Time = ifelse(Time == "", NA, Time), Event = ifelse(Event == "", NA, Event)) %>% mutate(Season_Improvement = ifelse(!is.na(Time) & is.na(Season_Improvement), 0, Season_Improvement))

head(imputed)
plot_bar <- imputed %>% filter(!is.na(Event)) %>% mutate(Best = ifelse(Event %in% c("100 Y Free", "50 Y Free", "100 Y Back", "200 Y Back", "200 Y IM"), 1, 0))
ggplot(data = plot_bar, aes(x=as.factor(Event), fill = as.factor(Best))) + 
  geom_bar() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  guides(fill = "none") +
  labs(title = "Distribution of Events in College", subtitle = "2020-2025", x ="events")
```

```{r}

summary_imp <- imputed %>% group_by(Date) %>% summarise(mean_imp = mean(Season_Improvement, rm.na = TRUE), max_imp = max(Season_Improvement, rm.na = TRUE)) %>% na.omit()

#get week summary for improvement so we can match it with weekly sleep and strain
summary_week <- summary_imp %>%
  mutate(week = floor_date(Date, "week")) %>%
  group_by(week) %>%
  summarise(mean_imp = mean(mean_imp),
            max_imp = max(max_imp))

summary_week$week = as.POSIXct.Date(summary_week$week)


```

## Sleep Metrics over the Years

```{r}
sleep = read.csv("sleeps.csv")
colnames(sleep) = c("cycle_start", "cycle_end", "cycle_tzone", "sleep_onset", "wake_onset", "performance", "resp_rate", "asleep_dur", "in_bed_dur", "light_sleep", "deep_sleep", "rem", "awake", "sleep_need", "sleep_debt", "efficiency", "consistency", "nap")

sleep_data <- date_cleaning(sleep)


melted_data <- sleep_data %>% select(-c(cycle_start, cycle_end, sleep_onset, nap)) %>% pivot_longer(cols = -c(wake_onset, grade, days_since), names_to = "metric", values_to= "value")

```


```{r  warning=FALSE}

metric_plot <- melted_data %>% mutate(reference = ifelse(month(wake_onset) == 8, "pre season", ifelse(month(wake_onset) %in% c(5,6,7), "summer", ifelse(month(wake_onset) == 2, "conference", NA))))

summary(metric_plot)

metrics = c("efficiency", "performance", "sleep_debt", "rem")
for (i in 1:4){
  g_metric <- ggplot(metric_plot[metric_plot$metric == metrics[i],], # Set data
              aes(x = days_since, y = value, color = grade))+ # Set aesthetics
  #geom_jitter(alpha = 0.2, height =0.1, aes(fill = metric))+ # Add points 
    
  geom_vline(xintercept = metric_plot[metric_plot$reference == "pre season",]$days_since, linetype = "dotted", color = "blue") + # Add vertical reference line
    
  geom_vline(xintercept = metric_plot[metric_plot$reference == "conference",]$days_since, linetype = "dotted", color = "green") + # Add vertical reference line
    
  geom_smooth(se = FALSE) + # Add smooting line
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + # Turn off grid
  dark_theme_minimal() + # Set theme
  labs(title = paste0("Whoop Metrics -- ", metrics[i]), # Set labels
       color = "Year") +
      annotate("text", x = 110, y = max(metric_plot[metric_plot$metric == metrics[i] & !is.na(metric_plot$value),]$value) -5, label = "pre season", color = "blue") + # Label for vertical line
    annotate("text", x = 290, y = max(metric_plot[metric_plot$metric == metrics[i] & !is.na(metric_plot$value),]$value) -5, label = "conference", color = "green") + # Label for vertical line
  guides(fill = "none") 

# Generate plot
print(g_metric)

}
# Turn off dark mode
invert_geom_defaults()
```
## Sleep on Performance
```{r warning=FALSE}

### Sleep

plot_sleep = melted_data %>%  filter(metric %in% c("asleep_dur", "rem", "deep_sleep")) %>% mutate(week = floor_date(wake_onset, "week")) %>%
  group_by(week, metric) %>%
  summarise(mean = mean(value, na.rm = TRUE))

plot_sleep$week = as.Date(plot_sleep$week)
summary_week$week = as.Date(summary_week$week)

merged = merge(summary_week, plot_sleep, by = "week")

g_sleep1 = ggplot(merged, # Set data
              aes(y = mean, x = max_imp, color = metric)) + geom_smooth() +
  labs(x=" Max Performance Improvement %", y="Metric Mean (mins)", title = "The effect of sleep on max performance improvement") # Set aesthetics

g_sleep2 = ggplot(merged, # Set data
              aes(y = mean, x = mean_imp, color = metric)) + geom_smooth() +
  labs(x="Mean Performance Improvement %", y="Metric Mean (mins)", title = "The effect of sleep on mean performance improvement") # Set aesthetics
g_sleep1
g_sleep2
```
At the highest values of deep and REM sleep, performance either increased the most or decreased the most. This could be explained by the fact more sleep has a positive effect of performance but also more sleep can indicate some other underlying problem such as sickness or fatigue. 

## Strain on Performance

```{r warning=FALSE}
### Strain
strain_week = plot_data %>% select(wake_onset, day_strain) %>% mutate(week =floor_date(wake_onset, "week")) %>%
  group_by(week) %>%
  summarise(strain_mean = mean(day_strain, na.rm = TRUE))

strain_week$week = as.Date(strain_week$week)
merged1 = merge(summary_week, strain_week,  by = "week")

colnames(strain_week)
g_strain1 = ggplot(merged1, # Set data
              aes(y = strain_mean, x = mean_imp)) + geom_smooth() +
  labs(x="MeanPerformance Improvement %", y= "Mean Day Strain", title = "The effect of strain on mean performance improvement") # Set aesthetics
g_strain1

g_strain2 = ggplot(merged1, # Set data
              aes(y = strain_mean, x = max_imp)) + geom_smooth() +
  labs(x="Max Performance Improvement %", y= "Mean Day Strain", title = "The effect of strain on max performance improvement") # Set aesthetics
g_strain2

```
Interestingly, there is not a linear relationship between strain and % improvement. It shows that a mean daily strain in the week leading up to my best overall performances (mean) is not the least but a little above, meaning I still kept some intensity in my workouts.

When we look at max performance improvement, which is looking at when I do the best in my best event compared to the season, for improvements 3% or better the strain in consistently around 14. But if the week strain is too low (around 11) or too high (around 15.5), then my best improvement drops off.



## Pre-Competition Workouts
```{r warning = FALSE}
conference_dates = c(
  "Freshman" = "02-25-2021", 
  "Sophomore" = "02-22-2022",
  "Junior" = "02-13-2023",
  "Senior" = "02-22-2024")

midseason_dates = c(
  "Freshman" = "11-20-2020", 
  "Sophomore" = "11-20-2021", 
  "Junior" = "11-17-2022",
  "Senior" = "11-16-2023",
  "Grad" = "11-21-2024")


workouts = read.csv("workouts.csv")
workouts = workouts[,c(1:16)]
colnames(workouts) = c("cycle_start", "cycle_end", "cycle_tzone", "workout_start", "workout_end", "dur", "activity", "activity_strain", "cals", "max_hr", "av_hr", "zone1", "zone2", "zone3", "zone4", "zone5")

workouts$date = as.Date(workouts$workout_start)

workouts = workouts %>%
  mutate(workout_start = ymd_hms(workout_start), workout_end = ymd_hms(workout_end)) %>%
    mutate(morning = ifelse(format(workout_start, "%H:%M:%S") < "12:00:00", 1, 0))

sessions <- workouts %>%
  select(morning, activity, date, dur, activity_strain) %>% 
  group_by(date) %>%
  summarise(
    total_dur = sum(dur),
    max_strain = max(activity_strain),
    count_morning = sum(morning == 1),
    count_afternoon = sum(morning == 0)
  )

activities = workouts %>%
  group_by(date, activity) %>%
  summarise(count = n()) %>%
  ungroup() %>%   pivot_wider(names_from = activity, values_from = count, values_fill = list(count = 0))  %>%
  rename_at(vars(-date), ~ paste0(., "_count")) %>% select(date, Swimming_count, Weightlifting_count, Activity_count, Cycling_count) %>% mutate(Dryland_count = Weightlifting_count + Activity_count + Cycling_count) %>% select(date, Swimming_count, Dryland_count)


zones = workouts %>% group_by(date) %>% summarise( zone1 = sum(zone1), zone2 = sum(zone2), zone3 = sum(zone3), zone4 = sum(zone4), zone5 = sum(zone5))

 
workout_data = merge(merge(activities, sessions, by ="date"), zones, by = "date")
workout_data$date = as.Date(workout_data$date)
workout_data <-  workout_data %>% mutate(grade = sapply(date, grade_mapping)) 


pre_meet = workout_data %>% na.omit() %>% mutate(conf_date = mdy(conference_dates[grade]), midseason_date = mdy(midseason_dates[grade])) %>% filter(conf_date-21  <= date & date <= conf_date | (midseason_date-21  <= date & date <= midseason_date)) %>% 
  mutate(days_til1 = (as.numeric(difftime(conf_date, date, units = "days"))))%>% 
  mutate(days_til2 = (as.numeric(difftime(midseason_date, date, units = "days")))) %>% 
  rowwise() %>% mutate(days_til = ifelse(days_til2 >= 0, min(days_til2, days_til1), max(days_til2, days_til1))) %>% ungroup() %>% select(-days_til1, -days_til2) %>% mutate(event =ifelse( abs(difftime(conf_date, date)) > abs(difftime(midseason_date, date)),  "conference", "midseason"))

 
```

```{r warning = FALSE}

metrics = c("max_strain", "total_dur")

  pre_plot = pre_meet %>% select(days_til, max_strain, total_dur, event, grade) %>% pivot_longer(cols = -c(days_til, event, grade), names_to = "metric", values_to = "value") 
  
  g_preconf <- ggplot(pre_plot[pre_plot$metric == metrics[1] & pre_plot$event == "conference",], # Set data
              aes(x = days_til, y = value, color = grade) )+ # Set aesthetics

    
  geom_smooth(se = FALSE) + # Add smooting line
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + # Turn off grid
  dark_theme_minimal() + # Set theme
  labs(title = "Whoop Metrics -- Pre Conference", subtitle = metrics[1], # Set labels
       color = "Year", "Max Whoop Strain per Day") +
  scale_x_reverse()

# Generate plot
print(g_preconf)
  
  g_preconf1 <- ggplot(pre_plot[pre_plot$metric == metrics[2] & pre_plot$event == "conference",], # Set data
              aes(x = days_til, y = value, color = grade) )+ # Set aesthetics

    
  geom_smooth(se = FALSE) + # Add smooting line
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + # Turn off grid
  dark_theme_minimal() + # Set theme
  labs(title = "Whoop Metrics -- Pre Conference", subtitle = metrics[2], # Set labels
       color = "Year", y = "Mins per Day")+
  scale_x_reverse()

# Generate plot
print(g_preconf1)
```

This shows how many workouts changed when approaching two major meets this year. My most successful year, Senior year, I kept at least one of my practices with high intensity and dropped that intensity dramatically with around a week to go. There was a similar pattern with total workout duration per day, but the drop happened early around 10 days previous.

```{r warning = FALSE}
  g_premid <- ggplot(pre_plot[pre_plot$metric == metrics[1] & pre_plot$event == "midseason",], # Set data
              aes(x = days_til, y = value, color = grade) )+ # Set aesthetics

    
  geom_smooth(se = FALSE) + # Add smooting line
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + # Turn off grid
  dark_theme_minimal() + # Set theme
  labs(title = "Whoop Metrics -- Pre Mid", subtitle = metrics[1], # Set labels
       color = "Year", "Max Whoop Strain per Day")+
  scale_x_reverse()

# Generate plot
print(g_premid)


  
  g_premid1 <- ggplot(pre_plot[pre_plot$metric == metrics[2] & pre_plot$event == "midseason",], # Set data
              aes(x = days_til, y = value, color = grade) )+ # Set aesthetics

    
  geom_smooth(se = FALSE) + # Add smooting line
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + # Turn off grid
  dark_theme_minimal() + # Set theme
  labs(title = "Whoop Metrics -- Pre Mid", subtitle = metrics[2], # Set labels
       color = "Year", y = "Mins per Day")+
  scale_x_reverse()

# Generate plot
print(g_premid1)

```

There is a lot less significant pattern approaching mid-season as there is less emphasis on resting for this meet. Interesting my best two years, junior and senior, had completely different approaches to taper.

## EDA Findings

Based on my EDA and my knowledge of how I have swam over the years, there are certain factors that seem to affect my performance. I will use these derived variables for the time leading into meets

**From the sleep metrics:**
- REM sleep (week before)
- Deep sleep (week before)
- Sleep efficiency (week before)

**From workout metrics:**
- mean daily workout duration (2 weeks before)
  - swims
  - mornings
- mean daily strain (2 weeks before)
- max daily workout strain (2 weeks before)
- A:C ratio for each aerobic zone (7:28)

**From physiology metrics:**
- mean max daily HR
- mean RHR
- mean HRV




## Top 5 Events

```{r Get top 5 event results as output for model}
library(readxl)
top5 <- read_excel("swim_top5.xlsx") %>%
  group_by(Date, Event) %>%
  mutate(SwimNum = dense_rank(desc(row_number()))) 

back2 <- read.delim("./back2.txt", comment.char="#")

back2 <- back2 %>% select(-X) %>% separate(Time, into = c("Min", "Sec"), sep = ":") %>% mutate(Time = as.numeric(Min)*60 + as.numeric(Sec), Event = "200 Y Back", Date = mdy(Date)) %>% select(-Min, -Sec) %>% 
  group_by(Date, Event) %>%
  mutate(SwimNum = dense_rank(desc(row_number()))) 

top5 <- rbind(top5, back2)

top5 <- top5  %>% group_by(Event) %>% arrange(Event, Date, SwimNum)
pb = 1000
pb_col = c() 
imp_col = c()
prev_event = ""

for (i in 1:nrow(top5)) {
  row <- top5[i, ]
  event = row$Event
  
  if (event == prev_event & row$Time <= pb){
      pb_col = c(pb_col, 1)
      
      imp = -(top5[i, ]$Time - pb)/top5[i, ]$Time
      
      imp_col = c(imp_col, imp)
      
      pb = top5[i, ]$Time
  } else if(event != prev_event){ #first time swam new event
    
      pb_col = c(pb_col, 1)
      imp_col = c(imp_col, 0)
      pb = top5[i, ]$Time
  } else {
      pb_col = c(pb_col, 0)
      imp = -(top5[i, ]$Time - pb)/top5[i, ]$Time
      imp_col = c(imp_col, imp)

  }
  
  prev_event = row$Event
  
  
}

top5$PB = pb_col
top5$Imp= imp_col
head(top5)

top5 <- top5 %>% mutate(Tapered = ifelse(str_detect(Meet, "(Conference)|(Invitational)|(3 session 11/20)"), 1, 0), Tapered = ifelse(Meet== "Rice Invitational", 0, Tapered))


```


# 

## Final Dataframe Prep

```{r}
library(mice)
all_whoop <- inner_join(sleep_data, plot_data)
all_whoop$date = as.Date(all_whoop$wake_onset)

all_whoop <- left_join(all_whoop, workout_data)
summary(all_whoop)

all_whoop <- all_whoop %>% mutate(date = as.Date(wake_onset))

all_whoop <- all_whoop %>% 
  mutate(across(34:44, ~ifelse(is.na(.), 0, .))) #impute NA workout data with 0-- I did no workout


#Derive some more variables
all_whoop <- all_whoop %>% arrange(date) %>% mutate(waking_hours = as.numeric(difftime(time1= as.POSIXct(wake_onset), time2 =as.POSIXct(sleep_onset), units = 'hours')))



```

## Imputation

```{r Dealing with NAs}
#Lets remove some NAs
#Need to impute sleep consistency, consistency and day strain
all_whoop <- all_whoop %>% filter(!is.na(grade)) %>% select(-blood_oxygen, -resp_rate, -skin_temp)

#Separate into numeric and non numeric
numeric <- all_whoop %>% select_if(is.numeric)
not_numeric <- all_whoop %>% select_if(~ !is.numeric(.))

correlation_matrix <- cor(numeric, use = "complete.obs")
which(correlation_matrix == 1, arr.ind = TRUE) #Lets look for perfect correlations


#Save for later
daily_vars = colnames(all_whoop %>% select(-c(cycle_start, cycle_end, sleep_onset, wake_onset, `awake`, `nap`, `days_since`, `sleep_consistency`)))

```

We can see that sleep consistency and consistency, and awake and awake_dur are perfectly correlated so we don't need both of them. Let's remove sleep consistency


```{r  warning=FALSE}
#Remove sleep consistency
numeric <- numeric %>% select(-sleep_consistency, -awake)

#Impute remaining NAs in consistency and day strain with MICE
imputed_data <- mice(numeric, m = 5, method = 'pmm', maxit = 50, seed = 500, print=FALSE)

sum(is.na(imputed_data))

complete_numeric <- complete(imputed_data, 1)

sum(is.na(complete_numeric))

whoop_imputed <- cbind(not_numeric, complete_numeric)
sum(is.na(whoop_imputed))

dim(whoop_imputed)[1] == dim(all_whoop)[1] #they are still the same length
```

We have dealt with all NAs.


Now we can create all other derived variables:

## Derived Variables

### AC Ratio

```{r}


#Lets split metric-admin for AC ratio
#I want to look at aerobic zones, total_dur, counts and day strain
#Look over ratios: 3:7, 7:21

#met_data <- whoop_imputed %>% select(zone1, zone2, zone3, zone4, zone5, day_strain, Swimming_count, Dryland_count, total_dur, count_morning, count_afternoon) 

whoop_imputed <- whoop_imputed %>% select(-nap)
met_data <- whoop_imputed %>% select(-c(cycle_start, cycle_end, sleep_onset, wake_onset, date, days_since, grade))

#admin_dat <- whoop_imputed %>% select(-c(zone1, zone2, zone3, zone4, zone5, day_strain, Swimming_count, Dryland_count, total_dur, count_morning, count_afternoon)) %>% rename(days = days_since)


admin_dat <- whoop_imputed %>% select(c(cycle_start, cycle_end, sleep_onset, wake_onset, date,days_since, grade))%>% rename(days = days_since)

dim(admin_dat)
dim(met_data)

```


```{r AC ratio}
sum_days <- function(met_data, admin_dat, met, days){
  #'
  #' This function calculates the sum of a given metric over a 
  #' period of time.
  #'
  #' @param met_data Data frame of metrics over a given period
  #' @param admin_data Data frame of adminstration variables, with 
  #' columns of players, indicating player id, and days, indicating the
  #' day each metric was measured on
  #' @param met The metric to calculate the sum for
  #' @param days The time window to calculate the sum for
  #' 
  #' @return A vector containing the summed variable over the 
  #' previous days period for each row.
  #'
  #' @examples
  #' g_data <- data.frame(distance = c(1,2,3,4))
  #' a_data <- data.frame(players = c(1,1,1,1),
  #'                          days = c(1,2,3,4))
  #' sum_res <- sum_days(met_data = g_data,
  #'                     admin_dat = a_data,
  #'                     met = "distance",
  #'                     days = 2)
  #'
  
  # Create empty vector to store results
  sum_vec <- rep(NA, nrow(met_data))
  # For each row of results data frame
  for(i in 1:nrow(met_data)){
    # Extract previous data which falls in given window, for given metric
    temp_vec <- met_data[which(admin_dat$days < admin_dat$days[i] &
                               admin_dat$days >= (admin_dat$days[i] - days)),c(met)]
    # If the length of this vector is at least 1
    if(length(temp_vec) >= 1){
      # Calculate sum of the values and assign to result vector
      sum_vec[i] <- sum(temp_vec, na.rm = T)
    }
  }
  # Return result vector
  return(sum_vec)
}

```


```{r}
ac_ratio <- function(met_data, admin_dat, met, days_1, days_2){
  #'
  #' This function calculates the acute:chronic ratio for a given 
  #' metric over a set time period. 
  #' 
  #' @param met_data A data frame containing metric values
  #' @param admin_data Data frame of adminstration variables, with 
  #' columns of players, indicating player id, and days, indicating the
  #' day each metric was measured on.
  #' @param met The metric of interest
  #' @param days_1 - The number of days to use for the acute window
  #' @param days_2 - The number of days to use for the chronic window
  #'
  #' @return A vector of the acute chronic window corresponding to each
  #' row in met_data and admin_db
  #'
  #' @examples
  #' g_data <- data.frame(distance = c(1,2,3,4))
  #' a_data <- data.frame(players = c(1,1,1,1),
  #'                          days = c(1,2,3,4))
  #' ac_res <- ac_ratio(met_data = g_data,
  #'                    admin_dat = a_data,
  #'                    met = "distance",
  #'                    days_1 = 1,
  #'                    days_2 = 2)
  #'
  
  # Calculate the sum of values for the acute window
  sum_vec_1 <- sum_days(met_data, admin_dat, met = met, days = days_1)
  # Calculate the sum of values for the chronic window
  sum_vec_2 <- sum_days(met_data, admin_dat, met = met, days = days_2)
  # Modify the chronic vector to same level as acute
  mod_sum_vec_2 <- sum_vec_2 * (days_1/days_2)
  # Calculate AC ratio
  ac_ratio <- sum_vec_1/mod_sum_vec_2
  # Return ac ratio vector
  return(ac_ratio)
}



```



```{r}
# Create empty data frames to store our results
sum_21_res <- sum_2_res <- sum_7_res <- sum_3_res <- sum_14_res <- ac_7_21_res <- ac_3_7_res <- ac_28_56_res <-  as.data.frame(matrix(NA, nrow = nrow(met_data), ncol = ncol(met_data)))


# For each column in the gps data
for(i in 1:ncol(met_data)){
  # Calculate the sum over 3 days
  sum_2_res[,i] <- sum_days(met_data, # Set GPS data
                            admin_dat,  # Set Admin data
                            met = names(met_data)[i], # Set metric to use
                            days = 2) # Set window size
  
  
  sum_3_res[,i] <- sum_days(met_data, # Set GPS data
                            admin_dat,  # Set Admin data
                            met = names(met_data)[i], # Set metric to use
                            days = 3) # Set window size
  
  # Calculate the sum over 7 days
  sum_7_res[,i] <- sum_days(met_data, # Set GPS data
                             admin_dat,  # Set admin data
                             met = names(met_data)[i], # Set metric to use
                             days = 7) # Set window size
  
  # Calculate the sum over 14 days
  sum_14_res[,i] <- sum_days(met_data, # Set GPS data
                             admin_dat, # Set admin data
                             met = names(met_data)[i], # Set metric to use
                             days = 14) # Set window size
  
  sum_21_res[,i] <- sum_days(met_data, # Set GPS data
                            admin_dat,  # Set Admin data
                            met = names(met_data)[i], # Set metric to use
                            days = 21) # Set window size
  
  # Calculate the A:C ratio for 7:21
  ac_7_21_res[,i] <- ac_ratio(met_data, # Set GPS data
                              admin_dat, # Set admin data
                              met = names(met_data)[i], # Set metric to use
                              days_1 = 7, # Set acute window
                              days_2 = 21) # Set chronic window
  
  # Calculate the A:C ratio for 3:7
  ac_3_7_res[,i] <- ac_ratio(met_data, # Set GPS data
                               admin_dat, # Set admin data
                               met = names(met_data)[i], # Set metric to use
                               days_1 = 3, # Set acute window
                               days_2 = 7) # Set chronic window
  
  # Calculate the A:C ratio for 3:7
  ac_28_56_res[,i] <- ac_ratio(met_data, # Set GPS data
                               admin_dat, # Set admin data
                               met = names(met_data)[i], # Set metric to use
                               days_1 = 28, # Set acute window
                               days_2 = 58) # Set chronic window
  
}
```

Next up we need to fix the names of the result data frames we have calculated so that we can tell which metric is which:

```{r}
# Set names for 2 day sums
names(sum_2_res) <- paste(names(met_data), "_sum_2_days", sep="")
# Set names for 21 day sums
names(sum_21_res) <- paste(names(met_data), "_sum_21_days", sep="")
# Set names for 7 day sums
names(sum_7_res) <- paste(names(met_data), "_sum_7_days", sep="")
# Set names for 14 day sums
names(sum_14_res) <- paste(names(met_data), "_sum_14_days", sep="")
# Set names for 3 day sums
names(sum_3_res) <- paste(names(met_data), "_sum_3_days", sep="")
# Set names for A:C 7:21
names(ac_7_21_res) <- paste(names(met_data), "_ac_7_21", sep="")
# Set names for A:C 28:56
names(ac_3_7_res) <- paste(names(met_data), "_ac_3_7", sep="")
# Set names for A:C 28:56
names(ac_28_56_res) <- paste(names(met_data), "_ac_28_56", sep="")
```

Finally we want to rejoin all our data together so that we have a single data frame to work with:

```{r}
# Join data together
met_dat <- cbind.data.frame(met_data, sum_2_res,sum_3_res, sum_7_res, sum_14_res,sum_21_res, ac_3_7_res, ac_7_21_res, ac_28_56_res)

dim(met_dat)
dim(admin_dat)

all_whoop_1 <- cbind.data.frame(admin_dat, met_dat)

```


### Lags


```{r}
# sleep_met <- all_whoop_1 %>% select(in_bed_dur, awake_dur, asleep_dur, deep_sleep, rem, sleep_need, sleep_debt, sleep_performance, waking_hours)
# 
# sleep_admin <- all_whoop_1 %>% select(-c(in_bed_dur, awake_dur, asleep_dur, deep_sleep, rem, sleep_need, sleep_debt, sleep_performance, waking_hours))

#use whoop_imputed not whoop_all_1, we dont want to lag ac ratios
lag_met <- whoop_imputed %>% select(-c(cycle_start, cycle_end, sleep_onset, wake_onset, date, days_since, grade))

lag_admin <- whoop_imputed %>% select(c(cycle_start, cycle_end, sleep_onset, wake_onset, date,days_since, grade))

```


```{r}
library(matrixStats)

lag_metric_calculator <- function(metric_data, lagged_days = 3,
                                  metric = "in_bed_dur",
                                  include_current = TRUE){
  #'
  #' This function carries out calculations on lagged values of a given
  #' metric
  #' 
  #' @param metric_data The data frame containing the metrics of interest
  #' @param lagged_days How many lagged days to use in the calculations
  #' @param metric The metric to create the calculations for 
  #' @param include_current Should the current day be included in the 
  #' calculation
  #'
  #' @return This function returns a data frame containing the calculated mean,
  #' sum, max, and min for the set time period (lagged_days). It also calculates
  #' how many values were missing in that time period. 
  #'
  
  # Extract vector to lag
  met_vec <- unlist(metric_data[,metric])
  
  if(include_current){
    # Create empty data frame to store results
    res_data <- matrix(NA, nrow = nrow(metric_data), ncol = lagged_days + 1)
    
    # Add current column to res_data
    res_data[,lagged_days + 1] <- met_vec
  } else{
     # Create empty data frame to store results
    res_data <- matrix(NA, nrow = nrow(metric_data), ncol = lagged_days)
  
  }
  
  
  # Loop through days and calculate lagged values
  for(i in 1:lagged_days){
    # Store lagged values
    res_data[,i] <- lag(x = met_vec, n = i)
  }
  
  # Calculate results for each day:
  mean_val <- rowMeans(res_data, na.rm = TRUE) # Calculate mean value
  sum_val <- rowSums(res_data, na.rm = TRUE) # Calculate sum value
  max_val <- rowMaxs(res_data, na.rm = TRUE) # Calculate max value
  min_val <- rowMins(res_data, na.rm = TRUE) # Calculate min value
  
  # Combine results vector
  results <- cbind.data.frame(mean_val, sum_val, max_val, min_val)
  
  # Handle the infinite values for max and min calculations
  results$max_val[is.infinite(results$max_val)] <- NA
  results$min_val[is.infinite(results$min_val)] <- NA
  
  
  names(results) <- paste(names(results), "_lag_", as.character(lagged_days+1), "_res", sep="")
  
  # Return results
  return(results)
}
```

```{r}

# sleep_metrics = c('in_bed_dur', 'awake_dur', 'asleep_dur', 'deep_sleep', 'rem', 'sleep_need', 'sleep_debt', 'sleep_performance', 'waking_hours')
# sleep_lags = data.frame()
# 
# lag_3_res <- lag_7_res <- lag_14_res <-  as.data.frame(matrix(NA, nrow = nrow(sleep_met), ncol = 5))


lag_3_res <- lag_7_res <- lag_14_res <-  lag_2_res <-  lag_10_res <- lag_21_res <-  as.data.frame(matrix(NA, nrow = nrow(lag_met), ncol = 4))
lags = data.frame()


for (i in seq(1:ncol(lag_met))){
  lag_3_res <- lag_7_res <- lag_14_res <-  lag_2_res <-  lag_10_res <- lag_21_res <- as.data.frame(matrix(NA, nrow = nrow(lag_met), ncol = 4))

  lag_3_res <- lag_metric_calculator(lag_met, lagged_days = 2,
                                    metric = colnames(lag_met)[i],
                                    include_current = TRUE)
  lag_7_res <- lag_metric_calculator(lag_met, lagged_days = 6,
                                    metric = colnames(lag_met)[i],
                                    include_current = TRUE)
  lag_14_res <- lag_metric_calculator(lag_met, lagged_days = 13,
                                    metric = colnames(lag_met)[i],
                                    include_current = TRUE)
  lag_2_res <- lag_metric_calculator(lag_met, lagged_days = 1,
                                    metric = colnames(lag_met)[i],
                                    include_current = TRUE)
  lag_10_res <- lag_metric_calculator(lag_met, lagged_days = 9,
                                    metric = colnames(lag_met)[i],
                                    include_current = TRUE)
  lag_21_res <- lag_metric_calculator(lag_met, lagged_days = 20,
                                    metric = colnames(lag_met)[i],
                                    include_current = TRUE)

  # Set names for 3 day
  names(lag_3_res) <- paste( colnames(lag_met)[i], "_", names(lag_3_res),sep="")
  # Set names for 7 day
  names(lag_7_res) <- paste( colnames(lag_met)[i], "_", names(lag_7_res),sep="")
  # Set names for 14 day
  names(lag_14_res) <- paste( colnames(lag_met)[i], "_", names(lag_14_res),sep="")
  # Set names for 2 day
  names(lag_2_res) <- paste( colnames(lag_met)[i], "_", names(lag_3_res),sep="")
  # Set names for 10 day
  names(lag_10_res) <- paste( colnames(lag_met)[i], "_", names(lag_7_res),sep="")
  # Set names for 21 day
  names(lag_21_res) <- paste( colnames(lag_met)[i], "_", names(lag_14_res),sep="")
  
  if (nrow(lags) == 0){
    lags = cbind.data.frame(lag_3_res, lag_7_res, lag_14_res, lag_2_res, lag_10_res, lag_21_res)
  } else {
    lags = cbind.data.frame(lags, lag_3_res, lag_7_res, lag_14_res, lag_2_res, lag_10_res, lag_21_res)
  }
  
    
  
}
```


```{r}
# dim(sleep_met)
# dim(sleep_admin)
# dim(sleep_lags)

#all_whoop_2 <- cbind.data.frame(sleep_admin, sleep_met, sleep_lags)

dim(lag_admin)
dim(lag_met)
dim(lags)

all_whoop_2 <- cbind.data.frame(all_whoop_1, lags)
dim(all_whoop_2)

#sum(is.na(all_whoop_2))

#colnames(all_whoop_2)

# save(all_whoop_2, all_whoop_1, lags, whoop_imputed, file = "post_sleep.rda")
# load("post_sleep.rda")

```
Still got to look at HRs and whoop metrics
```{r}
library(naniar)
vis_miss(all_whoop_2[,50:100])

nrow(all_whoop_2[is.na(all_whoop_2$awake_dur_sum_3_days),]) #the missing data is just a few rows at the start of each season-- let's get rid of those

all_whoop_2 <- all_whoop_2 %>% na.omit()
sum(is.na(all_whoop_2))
```

### Date-Derived
```{r Date}

#Months to be categorical to see if a specific month has an impact
all_whoop_3 <- all_whoop_2 %>% mutate(month = as.character(month(date, label=TRUE)), 
                                    dow = wday(date),
                                    wake_up_hour = as.numeric(format(wake_onset, "%H")) + as.numeric(format(wake_onset, "%M"))/60,
                                    night_before_bed_time = as.numeric(format(sleep_onset, "%H")) + as.numeric(format(sleep_onset, "%M"))/60)#no need to lag, already lagged

#Now we can remove all the date specific metrics
all_whoop_3 <- all_whoop_3 %>% select(-c(cycle_start, cycle_end, wake_onset, sleep_onset))


names(all_whoop_3)[sapply(all_whoop_3, is.character)]
```

Now we have all whoop metrics prepped and ready for model. We can start adding outcome variables and any metrics to do with my swims. These will include a days_until var which indicates days until the closest meet



```{r}
library(tidyr)


duplicated_dates <- top5 %>%
  mutate(date = lapply(Date, function(x) x - 0:20),) %>%  # Create sequences of 0 to 21*86400 seconds (seconds in a day)
  unnest(date) %>% # Expand into multiple rows
  mutate(days_until = (as.numeric(difftime(Date, date, units = "days")))) %>% data.frame() %>% rename(Meet_Date = Date)

duplicated_dates$date = as.Date(duplicated_dates$date)

merged_df = merge(duplicated_dates, all_whoop_3, by = "date") %>% mutate(PB = as.integer(PB), Tapered = as.integer(Tapered)) %>% arrange(date)

#We now have duplicated rows up to days - 20 from a meet. This is because some meets I take my whoop off before I start. We want to keep the min value for days_until (closest whoop metric to the event)

filtered_df <- merged_df %>% group_by(Event, Meet, Time) %>% filter(days_until == min(days_until)) %>% ungroup()

dim(top5)[1] == dim(filtered_df)[1]

#daily_vars = colnames(merged_df[,c(11:37, 93:101)])

final_df = filtered_df %>% select(-c(date,days_until, Meet_Date,Meet, all_of(daily_vars))) #remove specific date, meet info and daily vars

filtered_df[filtered_df$Meet == "Sterkel Classic",]$days_until

head(final_df)
```
# Models
Now let's think about outcome variables. For the overall model I will use either PB or Imp

a. PB (1/0)- for this I will take out Time and Imp as these were used to calculate
b. Imp (%)- for this I will take out Time and PB as these were used to calculate

## Overall PB XGB
First, a logistic XGB with PB as the outcome
```{r}
library(fastDummies)
char_cols = names(final_df)[sapply(final_df, is.character)]
final_w_dummies <- dummy_cols(final_df, select_columns = c(char_cols), remove_selected_columns = TRUE) %>% data.frame()

save(final_w_dummies, final_df, file = "premodel.rda")


```

```{r eval=FALSE, echo=FALSE}
load("premodel.rda")
```



```{r}
library(xgboost)
library(caret) 



set.seed(0)

parts = createDataPartition(final_w_dummies$PB, p = .8, list = F)

label_1 = final_w_dummies %>% select(PB) 
premodel_1 = data.matrix(final_w_dummies %>% select(-Time, -Imp, -PB))


trainx = premodel_1[parts, ]
trainy = label_1[parts, ]
testx = premodel_1[-parts, ]
testy = label_1[-parts, ]


dtrain_1 <- xgb.DMatrix(data = as.matrix(trainx), label = trainy)
dtest_1 <- xgb.DMatrix(data = as.matrix(testx), label = testy)
all_1 <- xgb.DMatrix(data = as.matrix(premodel_1), label = label_1$PB)
# Create test data

#dtest_1 <- xgb.DMatrix(data = as.matrix(test_dat[,c(8:11,17:42)]), label = test_inj)


fit_1 <- xgboost(dtrain_1,  # Set data set to use
                 nrounds = 100, # Set number of rounds
               
               verbose = 1, # 1 - Prints out fit
                print_every_n = 20, # Prints out result every 20th iteration
               
               objective = "binary:logistic", # Set objective
               eval_metric = "auc",
               eval_metric = "error")



boost_preds_1 <- predict(fit_1, dtest_1) # Create predictions for xgboost model

# Convert predictions to classes, using optimal cut-off
boost_pred_class <- rep(0, length(boost_preds_1))
boost_pred_class[boost_preds_1 >= 0.5] <- 1

t <- table(boost_pred_class, testy) # Create table
confusionMatrix(t, positive = "1") # Produce confusion matrix
```

We will add a weight vector as we can see PB = 1 is a large minority class with only ~10% of swims being a PB
```{r}
mean(label_1$PB)
weight_vec <- rep(1, length(trainy))

#Assign higher weight to PBs
weight_vec[trainy == 1] <- length(trainy)/sum(trainy == 1)


fit_2 <- xgboost(dtrain_1,  # Set data set to use
                 nrounds = 100, # Set number of rounds
               
               verbose = 1, # 1 - Prints out fit
                print_every_n = 20, # Prints out result every 20th iteration
               
               objective = "binary:logistic", # Set objective
               eval_metric = "auc",
               eval_metric = "error",
               
               weight = weight_vec)



boost_preds_2 <- predict(fit_2, dtest_1) # Create predictions for xgboost model

#Create new cut_off
cut_off <- sum(trainy== 1)/length(trainy)


# Convert predictions to classes, using optimal cut-off
boost_pred_class <- rep(0, length(boost_preds_2))
boost_pred_class[boost_preds_2 >= cut_off] <- 1

t <- table(boost_pred_class, testy) # Create table
confusionMatrix(t, positive = "1") # Produce confusion matrix

```
This is a better model as it has a higher balanced accuracy and we got better predictions for a PB which is what is most important.

Now retrain with all data.



```{r}

weight_vec <- rep(1, length(label_1$PB))

#Assign higher weight to PBs
weight_vec[label_1$PB == 1] <- length(label_1$PB)/sum(label_1$PB == 1)

fit_1 <- xgboost(all_1,  # Set data set to use
                 nrounds = 100, # Set number of rounds
               
               verbose = 1, # 1 - Prints out fit
                print_every_n = 20, # Prints out result every 20th iteration
               
               objective = "binary:logistic", # Set objective
               eval_metric = "auc",
               eval_metric = "error",
               weight = weight_vec)
```


```{r}

source("a_insights_shap_functions.r")
# Calculate SHAP importance
shap_result_1 <- shap.score.rank(xgb_model = fit_1, 
                X_train =as.matrix(premodel_1),
                shap_approx = F)


# Calculate data for SHAP plot
shap_long_1 = shap.prep(shap = shap_result_1,
                           X_train = as.matrix(premodel_1), 
                           top_n = 20)
# Generate SHAP plot
plot.shap.summary(data_long = shap_long_1)
```


### Insights


For my overall PB model, the most important factor was the minimum value of my day strain over the previous 14 days. This would describe my lightest/recovery days for the 14 days leading into competition. Interestingly, both very high and very low values of this variable have strong predictive power when predicting PB but perhaps in the opposite way you would expect. High values of min day strain over 14 days, contribute to "higher values" of PBs (a PB) and low values of min day strain over 14 days, contribute to "lower values" of PBs. This perhaps indicates that I need to keep my recovery days still somewhat intense, which is interesting as you would expect that low intensity would lead to lower fatigue and perhaps better performance.

This is also somewhat reflected in min value of max HR of the 14 days preceding. Minimum max HR is similar to day strain in that I should keep my maximum HR somewhat high over the two weeks leading into a meet. 

The maximum value of zone 2 swimming 3 days preceding is inversely proportional meaning the less zone 2 swimming I do in the days immediately prior to a  meet, the higher value of "PBs". This is not surprising to me, as traditionally this is a maintenance zone and you don't need to be doing lots of low intensity swimming leading a meet, you need to keep it short and fast.

Unsurprisingly when looking at sleep, better sleep scores in days preceding meets led to better performance. The most notable being sum of REM sleep 2 days before as well as mean sleep efficiency 7 days before. Increased REM sleep can lead to improved muscle recovery and cognitive function, both of which would be essential to performing at your peak. Sleep efficiency is a Whoop derived metric so whilst we do not know its exact formation, it is likely it also considers REM sleep.


### Volcano
```{r}
# Extract SHAP values from SHAP result for the first model 
shap_data <- as.data.frame(shap_result_1$shap_score)
# Extract variable names from the SHAP data
vars <- names(shap_data)
# Create result matrix to store results
res_mat <- as.data.frame(matrix(NA, nrow = length(vars), ncol = 4))
# Scale results
s_train <- scale(premodel_1)
#For each variable
for(i in 1:length(vars)){
  # Calculate mean SHAP score for PBs
  res_mat[i,1] <- mean(shap_data[label_1$PB == 1, vars[i]])
  # Calcualte mean SHAP score for no PBs
  res_mat[i,2] <- mean(shap_data[label_1$PB == 0 , vars[i]])
  # Calculate mean scaled variable value for PBs
  res_mat[i,3] <- mean(s_train[label_1$PB == 1, vars[i]])
  # Calcualte mean scaled variable value for no PBs
  res_mat[i,4] <- mean(s_train[label_1$PB == 0 , vars[i]])
}

# Create column of variables
res_mat$var <- vars
```


```{r}
# Calculate difference between PB and no PBS samples for SHAP score
res_mat$shap_diff <- res_mat[,1] - res_mat[,2]
# Calculate difference between PB and no PB samples for scaled variables
res_mat$met_diff <- res_mat[,3] - res_mat[,4]

# Create color column as absolute value of SHAP differences and metric differences
res_mat$color_scale <- abs(res_mat$shap_diff) + abs(res_mat$met_diff)
```


We also want to label important variables in the plot. For this we do not want to label unimportant variables. So we will select variables which have:

A SHAP difference greater than 0.5
A SHAP difference greater than 0.1 and a metric difference greater than 1
A metric difference greater than 2

```{r}
# Create binary selected variable indicator
sel_vars <- (abs(res_mat$shap_diff) > 0.5) | 
            ((abs(res_mat$met_diff) > 1) & (abs(res_mat$shap_diff) > 0.5)) |
            (abs(res_mat$met_diff) > 2)
```

```{r  warning=FALSE}
library(ggrepel)
# Create plot
g_5 <- ggplot(res_mat, # Set dataset
              aes(x = met_diff, y = shap_diff, color= color_scale)) + # Set aesthetics
  geom_point() + # Add geom point for scatter plot
  theme(axis.line = element_line(colour = "black"), # Set axis line as black
        panel.grid.major = element_blank(), # Remove grid
        panel.grid.minor = element_blank(), # Remove grid
        panel.border = element_blank(), # Remove grid
        panel.background = element_blank()) + # Remove grid 
  labs(x = "Difference in scaled values", # Add labels to plot
       y = "Difference in SHAP values",
       title = "Volcano Plot - PB Variables") +
  geom_text_repel(data = res_mat[sel_vars,] , # Add labels to selected points using test_repel
                  aes(label = var, x = met_diff, y = shap_diff),
                  color = "white", inherit.aes = FALSE,
                  size =3) +
  scale_color_gradient(low = "red", # Scale color gradient
                        high = "yellow",
                        aesthetics = "colour") +
  dark_theme_bw() + # Dark theme
  guides(color = FALSE) # Turn off color legend


g_5
```


A different visualisation shows the same variables are important in making this prediction.


## Overall Improvement XGB

Now let's try on % Imp
```{r warning=FALSE}

label_1 = final_w_dummies %>% select(Imp) 
premodel_1 = data.matrix(final_w_dummies %>% select(-Time, -Imp, -PB))

trainx = premodel_1[parts, ]
trainy = label_1[parts, ]
testx = premodel_1[-parts, ]
testy = label_1[-parts, ]

dtrain_1 <- xgb.DMatrix(data = as.matrix(trainx), label = trainy)
dtest_1 <- xgb.DMatrix(data = as.matrix(testx), label = testy)
all_1 <- xgb.DMatrix(data = as.matrix(premodel_1), label = label_1$Imp)


#dtest_1 <- xgb.DMatrix(data = as.matrix(test_dat[,c(8:11,17:42)]), label = test_inj)


fit_2 <- xgboost(all_1,  # Set data set to use
                 nrounds = 100, # Set number of rounds
               
               verbose = 1, # 1 - Prints out fit
                print_every_n = 20) # Prints out result every 20th iteration
               
               # eval_metric = "auc",
               # eval_metric = "error")

```
```{r}
# Calculate SHAP importance
shap_result_2 <- shap.score.rank(xgb_model = fit_2, 
                X_train =as.matrix(premodel_1),
                shap_approx = F)


# Calculate data for SHAP plot
shap_long_2 = shap.prep(shap = shap_result_2,
                           X_train = as.matrix(premodel_1), 
                           top_n = 20)
# Generate SHAP plot
plot.shap.summary(data_long = shap_long_2)
```

### Insights


I think this model is a little less useful than the previous one with the logistic output as we can see less segmentation between variables dictating "good" and "bad" performance, which makes sense as this response variable is % improvement which is on a scale.

The most important factor here is the logistic variable "Tapered". This is exactly what we'd expect as it shows 1 at target and championship meets where we would expect to drop the most time. This is somewhat promising that our current taper is effective.

From this SHAP, the variables I want to analyse that show divide between time drops and time adds are sleep debt and in bed duration, swim counts and zone 1 swimming.

The sleep variables, sleep debt 7:21 AC ratio and max duration in bed for 3 days preceding show sleep behaviour immediately prior to competition. When 7:21 sleep debt is less then there are higher values of % improvement and when max duration in bed 3 days prior to competition is low, I tend to see reduced performance. This follows our previous model that sleep immediately prior to competition is important for me.

For the swim counts AC ratio 7:21, it shows that when this value is low then there is increased % improvement meaning that the week following my competition should have fewer training sessions than than 2 and 3 weeks out.

For zone 1 swimming, which is similar to zone 2 and for maintenance and described as low intensity aerobic, I see marginal improvement when the max value for this is lower over the 3 days preceding competition. This follows our previous model that described a similar pattern for zone 2, indicating that I do not need a lot of slow, aerobic swimming immediately prior to meets.



```{r}
top100s <- final_df %>% filter(Event == "100 Y Back" | Event == "100 Y Free") %>% select(-Event)
top200s <- final_df %>% filter(Event == "200 Y IM" | Event == "200 Y Back") %>% select(-Event)
top50 <- final_df %>% filter(Event == "50 Y Free") %>% select(-Event)
top_free <- final_df %>% filter(Event == "50 Y Free" | Event == "100 Y Free") %>% select(-Event)
  
```

## 50s Imp XGB

```{r 50s Model}

label_50 = top50 %>% select(Imp) 
premodel_50 = data.matrix(top50 %>% select(-Time, -Imp, -PB))
all_50 <- xgb.DMatrix(data = as.matrix(premodel_50), label = label_50$Imp)
# Create test data

#dtest_1 <- xgb.DMatrix(data = as.matrix(test_dat[,c(8:11,17:42)]), label = test_inj)


fit_50 <- xgboost(all_50,  # Set data set to use
                 nrounds = 100, # Set number of rounds
               
               verbose = 1, # 1 - Prints out fit
                print_every_n = 20) # Prints out result every 20th iteration
               # 
               # eval_metric = "auc",
               # eval_metric = "error")
```
```{r}
# Calculate SHAP importance
shap_result_50 <- shap.score.rank(xgb_model = fit_50, 
                X_train =as.matrix(premodel_50),
                shap_approx = F)


# Calculate data for SHAP plot
shap_long_50 = shap.prep(shap = shap_result_50,
                           X_train = as.matrix(premodel_50), 
                           top_n = 20)
# Generate SHAP plot
plot.shap.summary(data_long = shap_long_50)
```

#### Insights


For the 50 (sprint event), the variables that have a positive correlation with % Improvement are:

- min REM sleep 2 weeks preceding (weakly)

The variables that have a negative correlation are:

- sum of light sleep in the 3 days preceding
- sleep debt 7:21 AC ratio
- mean number of swims 3 days preceding 

## 100s Imp XGB

```{r 100s Model}

label_100 = top100s %>% select(Imp) 
premodel_100 = data.matrix(top100s %>% select(-Time, -Imp, -PB))
all_100 <- xgb.DMatrix(data = as.matrix(premodel_100), label = label_100$Imp)
# Create test data

#dtest_1 <- xgb.DMatrix(data = as.matrix(test_dat[,c(8:11,17:42)]), label = test_inj)


fit_100 <- xgboost(all_100,  # Set data set to use
                 nrounds = 100, # Set number of rounds
               
               verbose = 1, # 1 - Prints out fit
                print_every_n = 20) # Prints out result every 20th iteration
               
               # eval_metric = "auc",
               # eval_metric = "error")
```
```{r}
# Calculate SHAP importance
shap_result_100 <- shap.score.rank(xgb_model = fit_100, 
                X_train =as.matrix(premodel_100),
                shap_approx = F)


# Calculate data for SHAP plot
shap_long_100 = shap.prep(shap = shap_result_100,
                           X_train = as.matrix(premodel_100), 
                           top_n = 20)
# Generate SHAP plot
plot.shap.summary(data_long = shap_long_100)
```

#### Insights


For the 100 (sprint event that requires a little more aerobic capacity than the 50), the variables that have a positive correlation with % Improvement are:

- maximum day strain 14 days preceding (weakly)

The variables that have a negative correlation are:

- total zone 4 swimming 3 days preceding (weakly)
- zone 4 AC ratio 7:21
- mean value of zone 2 swimming for 3 days preceding
- number of morning swims AC ratio 7:21

Nearly all important variables here are training related and indicate that as expected I need less aerobic swimming immediately before competition. But also need to reduce zone 4, most intense and lactate building, in the last week in comparison to 3 and 2 weeks out. With that I also need to reduce the number of morning swims in the week preceding a meet.
## 200s Imp XGB

```{r 200s Model}

label_200 = top200s %>% select(Imp) 
premodel_200 = data.matrix(top200s %>% select(-Time, -Imp, -PB))
all_200 <- xgb.DMatrix(data = as.matrix(premodel_200), label = label_200$Imp)
# Create test data

#dtest_1 <- xgb.DMatrix(data = as.matrix(test_dat[,c(8:11,17:42)]), label = test_inj)


fit_200 <- xgboost(all_200,  # Set data set to use
                 nrounds = 200, # Set number of rounds
               
               verbose = 1, # 1 - Prints out fit
                print_every_n = 20) # Prints out result every 20th iteration
               
               # eval_metric = "auc",
               # eval_metric = "error")
```
```{r}
# Calculate SHAP importance
shap_result_200 <- shap.score.rank(xgb_model = fit_200, 
                X_train =as.matrix(premodel_200),
                shap_approx = F)


# Calculate data for SHAP plot
shap_long_200 = shap.prep(shap = shap_result_200,
                           X_train = as.matrix(premodel_200), 
                           top_n = 20)
# Generate SHAP plot
plot.shap.summary(data_long = shap_long_200)
```

#### Insights


For the 200 (my most aerobic event), the variables that have a positive correlation with % Improvement are:

- sum of sleep performance 2 days preceding
- mean sleep performance 3 days preceding
- minimum value of light sleep 3 days preceding
- Taper or championship meets

The variables that have a negative correlation are:

- minimum value of sleep performance 7 days preceding
- swim number (the number of times I swam the same event at the same meet)
- mean value of sleep performance 7 days preceding

Interestingly, nearly all performance factors here are sleep related and very few are related to training approach.

## Free Imp XGB
```{r free Model}

label_f = top_free %>% select(Imp) 
premodel_f = data.matrix(top_free %>% select(-Time, -Imp, -PB))
all_f <- xgb.DMatrix(data = as.matrix(premodel_f), label = label_f$Imp)
# Create test data

#dtest_1 <- xgb.DMatrix(data = as.matrix(test_dat[,c(8:11,17:42)]), label = test_inj)


fit_f <- xgboost(all_f,  # Set data set to use
                 nrounds = 100, # Set number of rounds
               
               verbose = 1, # 1 - Prints out fit
                print_every_n = 20) # Prints out result every 20th iteration
               # 
               # eval_metric = "auc",
               # eval_metric = "error")
```
```{r }
# Calculate SHAP importance
shap_result_f <- shap.score.rank(xgb_model = fit_f, 
                X_train =as.matrix(premodel_f),
                shap_approx = F)


# Calculate data for SHAP plot
shap_long_f = shap.prep(shap = shap_result_f,
                           X_train = as.matrix(premodel_f), 
                           top_n = 20)
# Generate SHAP plot
plot.shap.summary(data_long = shap_long_f)
```

#### Insights


For freestyle events, the variables that have a positive correlation with % Improvement are:

- sum of sleep performance 2 days preceding
- mean sleep performance 3 days preceding
- minimum value of light sleep 3 days preceding
- Taper or championship meets

The variables that have a negative correlation are:

- minimum value of sleep performance 7 days preceding
- swim number (the number of times I swam the same event at the same meet)
- mean value of sleep performance 7 days preceding

Interestingly, nearly all performance factors here are sleep related and very few are related to training approach.



# Limitations

Overall, I think this was a successful exploratory study into myself and my performance based on Whoop metrics. However there were some limitations that I did my best to rectify.

Firstly, I do not have 100% complete Whoop data for reasons such as my Whoop dying or getting lost and most importantly, sometimes I remove it in the lead up to competitions to ensure the number don't affect my mindset. This caused some problems as it meant I didn't always have metric data on the day of competitions to join with race day. To overcome this I tracked back 20 days from each competition and joined with the minimum value of "days_until" meaning that I used the most recent data for each competition I had.

Secondly, there was a large imbalance in positive vs negative response variables for some of my models as generally in swimming you target 2-3 events per year and only expect to peak/PB at them. For this, I tried to adjust the weight vector and cut off value accordingly to accomodate for the lack of PBs in my data set.

Lastly, Whoop's derived metrics adapt to my current training period so it is hard to compare "strain" for this year at Notre Dame to my previous years at Rice as my training has changed to over the past year my resilience to different styles of training will also change.



# Conclusion and Application

Firstly, I would make an effort to keep my recovery days more intense for the two weeks leading up to a meet. This would probably mean that I wouldn't take a full day off.

Secondly, I would make the conscious effort to get good sleep specifically for the 3 days preceding the first day of the meet and practicing good sleep hygiene like going to bed at a consistent time.

Thirdly, as I am currently targeting 50/100s, I would focus on fewer, more intense sessions in the week leading up to the meet but there is no need to swim aerobically in the 3 days leading up.

An interesting extra finding that I found in this analysis, is that swim number was a significant factor in all my models. This is interesting because most swimmers have this mindset that they are a "morning" swimmer or an "evening." However, whilst this was an important factor in 50 and 100 improvement prediction, there was no clear pattern that my first (morning) or last (evening) swimmers were any better for predicting PBs.


